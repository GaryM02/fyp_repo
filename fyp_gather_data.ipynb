{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaryM02/fyp_repo/blob/main/fyp_gather_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKvAVjTbcgKz",
        "outputId": "1253b89a-4d4d-4981-bc46-c7b67bb3c4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjGSUMkyyZcC"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/PredictiveAnalytics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download compressed xml from https://ftp.ncbi.nlm.nih.gov"
      ],
      "metadata": {
        "id": "zTOGtpFOzfWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRwOH4mvDcHj"
      },
      "outputs": [],
      "source": [
        "_URLs = [\n",
        "    f\"https://ftp.ncbi.nlm.nih.gov/pubmed/baseline/pubmed24n{i:04d}.xml.gz\"\n",
        "    for i in range(1, 1220)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract Data From XML files"
      ],
      "metadata": {
        "id": "iuGuLgHS2mgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh_NCSzjDjrL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import concurrent.futures\n",
        "\n",
        "\n",
        "def download_file(url, download_dir):\n",
        "    \"\"\"Helper function to download a single file without progress tracking.\"\"\"\n",
        "    filename = os.path.join(download_dir, url.split(\"/\")[-1])\n",
        "\n",
        "    # Skip download if file already exists\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"{filename} already downloaded, skipping...\")\n",
        "        return filename\n",
        "\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=60) as response:\n",
        "            block_size = 4096  # Larger block size for faster writing\n",
        "\n",
        "            with open(filename, \"wb\") as file:\n",
        "                for data in response.iter_content(block_size):\n",
        "                    file.write(data)\n",
        "        return filename\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to download {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def download_pubmed_files(urls, download_dir=\"default\", max_workers=8):\n",
        "    \"\"\"Downloads PubMed files concurrently and saves only compressed files.\"\"\"\n",
        "    os.makedirs(download_dir, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel downloads\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit download tasks\n",
        "        futures = {\n",
        "            executor.submit(download_file, url, download_dir): url for url in urls\n",
        "        }\n",
        "\n",
        "        # Track and check completed downloads\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            url = futures[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {url}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqhJZpzjE6Gq"
      },
      "outputs": [],
      "source": [
        "download_pubmed_files(_URLs, \"Data/pubmed/compressed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import gzip\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gc\n",
        "\n",
        "class DataProcessor:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def append_to_parquet(self, new_data, parquet_file):\n",
        "        # Convert new data to Arrow Table\n",
        "        table = pa.Table.from_pandas(new_data)\n",
        "\n",
        "        # Check if file exists and has existing data\n",
        "        try:\n",
        "            existing_data = pq.read_table(parquet_file)\n",
        "            with pq.ParquetWriter(parquet_file, table.schema) as writer:\n",
        "                writer.write_table(existing_data)\n",
        "                writer.write_table(table)\n",
        "        except FileNotFoundError:\n",
        "            # If file doesn't exist, create it and write new data\n",
        "            pq.write_table(table, parquet_file)\n",
        "\n",
        "    def process_files_and_save(self, file_paths, output_filename):\n",
        "        articles = self.process_files(file_paths)\n",
        "        flattened_articles = [article for sublist in articles for article in sublist]\n",
        "        df = pd.DataFrame(flattened_articles)\n",
        "        self.append_to_parquet(df, output_filename)\n",
        "        print(f\"Data saved to {output_filename}\")\n",
        "\n",
        "    def process_file(self, file_path):\n",
        "        articles = self.extract_article_title_and_abstract(file_path)\n",
        "        return articles\n",
        "\n",
        "    def process_files(self, file_paths):\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            results = list(tqdm(executor.map(self.process_file, file_paths), total=len(file_paths)))\n",
        "        return results\n",
        "\n",
        "    def extract_article_title_and_abstract(self, filename):\n",
        "        articles = []\n",
        "        # Parse the XML content\n",
        "        with gzip.open(filename) as f:\n",
        "            try:\n",
        "                tree = ET.parse(f)\n",
        "                root = tree.getroot()\n",
        "            except ET.ParseError:\n",
        "                print(f\"Error parsing {filename}. Skipping...\")\n",
        "                return []  # Return an empty list on parse error to avoid issues\n",
        "\n",
        "            # Iterate over each PubmedArticle in the PubmedArticleSet\n",
        "            for article in root.findall(\"PubmedArticle\"):\n",
        "                # Extract ArticleTitle\n",
        "                title_elem = article.find(\".//ArticleTitle\")\n",
        "                article_title = title_elem.text if title_elem is not None else \"\"\n",
        "\n",
        "                # Extract AbstractText\n",
        "                abstract_elem = article.find(\".//AbstractText\")\n",
        "                abstract_text = abstract_elem.text if abstract_elem is not None else \"Na\"\n",
        "\n",
        "                # Store the extracted title and abstract in a dictionary\n",
        "                if abstract_text != \"Na\":  # Include only if abstract is available\n",
        "                    article_data = {\n",
        "                        \"title\": article_title,\n",
        "                        \"abstract\": abstract_text,\n",
        "                        \"embedding\": None  # Placeholder for embeddings\n",
        "                    }\n",
        "                    articles.append(article_data)\n",
        "\n",
        "        return articles\n",
        "\n",
        "    \"\"\"\n",
        "    Method to generate embeddings using transformers module\n",
        "\n",
        "    Parameters:\n",
        "    file_path: path to parquet file\n",
        "    model_name: name of the model to be used for embedding\n",
        "    batch_size: number of rows to process at a time\n",
        "\n",
        "    Returns: List of embeddings\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def get_embeddings(self, file_path, model_name, output_file, batch_size=5):\n",
        "        # Initialize tokenizer and model only once\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModel.from_pretrained(model_name)\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # Initialize the output Parquet writer schema only once\n",
        "        parquet_writer = None\n",
        "\n",
        "        # Stream the Parquet file row by row in batches\n",
        "        dataset = pq.ParquetFile(file_path)\n",
        "        num_rows = dataset.metadata.num_rows\n",
        "        for i in range(0, num_rows, batch_size):\n",
        "            # Read batch, convert abstracts to list\n",
        "            batch_df = dataset.read_row_group(i // batch_size).to_pandas()\n",
        "            abstracts = batch_df[\"abstract\"].fillna(\"\").tolist()\n",
        "            titles = batch_df[\"title\"].tolist()\n",
        "\n",
        "            if not abstracts:\n",
        "                continue\n",
        "\n",
        "            # Tokenize and get embeddings\n",
        "            inputs = tokenizer(abstracts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "            # Stream embeddings directly to Parquet\n",
        "            batch_output_df = pd.DataFrame({\n",
        "                \"title\": titles,\n",
        "                \"abstract\": abstracts,\n",
        "                \"embedding\": list(embeddings)\n",
        "            })\n",
        "            table = pa.Table.from_pandas(batch_output_df)\n",
        "\n",
        "            # Write directly to Parquet to avoid holding all embeddings in memory\n",
        "            if parquet_writer is None:  # Create writer with schema on the first batch\n",
        "                parquet_writer = pq.ParquetWriter(output_file, table.schema)\n",
        "\n",
        "            parquet_writer.write_table(table)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Processed {i + batch_size} / {num_rows} rows\")\n",
        "\n",
        "            # Explicitly clear memory\n",
        "            del batch_df, abstracts, titles, inputs, outputs, embeddings, batch_output_df\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            gc.collect()\n",
        "\n",
        "        # Close the writer after completing all batches\n",
        "        if parquet_writer:\n",
        "            parquet_writer.close()\n",
        "\n",
        "        print(f\"Embeddings saved to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z79DAEKl1Fzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths = [f\"Data/pubmed/compressed/pubmed24n{i:04d}.xml.gz\" for i in range(1, 5)]"
      ],
      "metadata": {
        "id": "VDYeIPFn2jAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using our DataProcessor class we can extract our data\n",
        "The data that is extracted will be saved in parquet format to reduce memory usage on disk."
      ],
      "metadata": {
        "id": "WlJ8bF0a7C84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create parquet file in Data/pubmed/parquet\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def create_directory(dir_path, filename=None):\n",
        "    \"\"\"\n",
        "    Create a new directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create directory if it doesn't exist\n",
        "        Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"Directory created at: {dir_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating directory: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "85nxJlvg7cWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_directory(\"Data/pubmed/parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im8oZk7J8ITN",
        "outputId": "705bc375-dd6b-49bb-8e1c-c509f17a4215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory created at: Data/pubmed/parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# process files and store in one parquet file\n",
        "processor = DataProcessor()\n",
        "processor.process_files_and_save(file_paths, \"Data/pubmed/parquet/pubmed.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWbZcSmk2yzA",
        "outputId": "ad7efc04-5943-4222-f807-c1956b457b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:06<00:00, 16.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to Data/pubmed/parquet/pubmed.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's examine the parquet file we created"
      ],
      "metadata": {
        "id": "SOq52uJM_d2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"Data/pubmed/parquet/pubmed.parquet\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJn8-gVg3HHv",
        "outputId": "8a5f756a-7cfc-47ef-d4ac-dfd08892f0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 71900 entries, 0 to 71899\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   title      71900 non-null  object\n",
            " 1   abstract   71900 non-null  object\n",
            " 2   embedding  0 non-null      object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Now Lets update our file with embedding usings some BERT model"
      ],
      "metadata": {
        "id": "uc2U9sIRB5pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths and parameters\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "batch_dataframe = []\n",
        "\n",
        "parquet_file = pq.ParquetFile('Data/pubmed/parquet/pubmed.parquet')"
      ],
      "metadata": {
        "id": "Z87BwJEOCBhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import gc\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "pwv5RO_9Wzug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import concurrent.futures\n",
        "\n",
        "def process_batch(batch, tokenizer, model_name):\n",
        "    \"\"\"Process a single batch: tokenize, generate embeddings, and prepare data for writing.\"\"\"\n",
        "    # Load model and tokenizer within the process to avoid GPU memory sharing issues\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "\n",
        "    batch_df = batch.to_pandas()\n",
        "    abstracts = batch_df[\"abstract\"].fillna(\"\").tolist()\n",
        "\n",
        "    # Skip empty abstracts\n",
        "    if not abstracts:\n",
        "        return None\n",
        "\n",
        "    # Tokenize and compute embeddings\n",
        "    inputs = tokenizer(abstracts, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "    # Append embeddings to DataFrame\n",
        "    batch_df[\"embedding\"] = list(embeddings)\n",
        "    new_data = batch_df[[\"title\", \"abstract\", \"embedding\"]]\n",
        "\n",
        "    # Convert to Arrow Table for writing\n",
        "    table = pa.Table.from_pandas(new_data)\n",
        "\n",
        "    # Free up memory\n",
        "    del batch_df, abstracts, inputs, outputs, embeddings, new_data\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    gc.collect()\n",
        "\n",
        "    return table\n",
        "\n",
        "def append_to_parquet(input_file, output_file, model_name, batch_size=100):\n",
        "    # Load schema from input file\n",
        "    schema_ = pq.read_schema(input_file)\n",
        "    schema_ = schema_.remove(2)  # Remove old 'embedding' column if it exists\n",
        "    schema_ = schema_.append(pa.field(\"embedding\", pa.list_(pa.float32())))  # Add new 'embedding' column\n",
        "\n",
        "    # Initialize output file if it doesn't exist\n",
        "    if not os.path.exists(output_file):\n",
        "        pq.write_table(pq.read_table(input_file), output_file)\n",
        "\n",
        "    # Initialize ParquetWriter with the schema\n",
        "    with pq.ParquetWriter(output_file, schema=schema_, use_dictionary=True) as writer:\n",
        "        # Load input file in batches\n",
        "        dataset = pq.ParquetFile(input_file)\n",
        "\n",
        "        # Use ProcessPoolExecutor for parallel processing\n",
        "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "            futures = []\n",
        "            for i, batch in enumerate(dataset.iter_batches(batch_size=batch_size)):\n",
        "                # Submit each batch to the executor for parallel processing\n",
        "                futures.append(executor.submit(process_batch, batch, AutoTokenizer.from_pretrained(model_name), model_name))\n",
        "\n",
        "            # Collect results as they complete\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                table = future.result()\n",
        "                if table:\n",
        "                    writer.write_table(table)\n",
        "\n",
        "                # Optional: Track progress\n",
        "                print(f\"Processed batch {i * batch_size} / {dataset.metadata.num_rows} rows\")\n",
        "\n",
        "    print(f\"Data appended successfully to {output_file}\")\n"
      ],
      "metadata": {
        "id": "AwH03vCDaI63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"Data/pubmed/parquet/pubmed_with_embeddings.parquet\""
      ],
      "metadata": {
        "id": "p8_dLQvnaLhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "append_to_parquet(\"Data/pubmed/parquet/pubmed.parquet\", \"Data/pubmed/parquet/pubmed_with_embeddings.parquet\", \"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "WSO28rQAXXK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"Data/pubmed/parquet/pubmed_with_embeddings.parquet\")\n",
        "df.info()\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa2HcGhcbhPl",
        "outputId": "41ee18be-b47a-4ba9-b4b2-c4203d6e9258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 71900 entries, 0 to 71899\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   title      71900 non-null  object\n",
            " 1   abstract   71900 non-null  object\n",
            " 2   embedding  71900 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1.6+ MB\n",
            "                                               title  \\\n",
            "0  Effects of axotomy on the trans-synaptic regul...   \n",
            "1  The effects of putative neurotransmitters on t...   \n",
            "2  A quantitative comparison of the formation of ...   \n",
            "3  Properties of soluble and particulate cysteine...   \n",
            "4  [Tissue lactates, pH and blood gasses in hiber...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  The effects of surgical transection of the pos...   \n",
            "1  Cultures established from mechanically dissoci...   \n",
            "2  The rat superior cervical sympathetic ganglion...   \n",
            "3  Some properties of cysteine sulfinate decarbox...   \n",
            "4  Periodic arousal in the garden dormouse is acc...   \n",
            "\n",
            "                                           embedding  \n",
            "0  [0.00979263, -0.31819227, 0.27969253, 0.082559...  \n",
            "1  [0.0021715146, -0.24222079, 0.10336114, -0.046...  \n",
            "2  [-0.0535728, -0.26550195, 0.07283524, 0.013576...  \n",
            "3  [0.06526394, -0.20548019, 0.15978041, 0.029673...  \n",
            "4  [0.26737976, -0.2454584, 0.38486928, 0.2165555...  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}